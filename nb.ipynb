{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from implementations import *\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data (2.23m)\n",
    "\n",
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (328135, 320)\n",
      "y_train shape:  (328135,)\n",
      "x_test shape:  (109379, 320)\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "\n",
    "print(\"x_train shape: \", x_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape) \n",
    "print(\"x_test shape: \", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = standardize(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "max_iters = 1000                            # max number of iterations \n",
    "threshold = 1e-8                            # threshold for stopping criterion\n",
    "gamma = 0.5                                 # step size\n",
    "initial_w = np.zeros((x_train.shape[1], 1)) # initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 802. GiB for an array with shape (328135, 328135) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/drudao/Desktop/EPFL/Year_I/MachineLearning/Projects/ML-Higgs-EPFL/nb.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/drudao/Desktop/EPFL/Year_I/MachineLearning/Projects/ML-Higgs-EPFL/nb.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Mean squared error gradient descent\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/drudao/Desktop/EPFL/Year_I/MachineLearning/Projects/ML-Higgs-EPFL/nb.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m w_mean_squared_error_gd, loss_mean_squared_error_gd \u001b[39m=\u001b[39m mean_squared_error_gd(y_train, x_train, initial_w, max_iters, gamma)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/drudao/Desktop/EPFL/Year_I/MachineLearning/Projects/ML-Higgs-EPFL/nb.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m rmse_tr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqrt(\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m loss_mean_squared_error_gd)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/drudao/Desktop/EPFL/Year_I/MachineLearning/Projects/ML-Higgs-EPFL/nb.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#rmse_te = np.sqrt(2 * compute_loss(y_test, x_test, w_mean_squared_error_gd))\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/EPFL/Year_I/MachineLearning/Projects/ML-Higgs-EPFL/implementations.py:50\u001b[0m, in \u001b[0;36mmean_squared_error_gd\u001b[0;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m     46\u001b[0m w \u001b[39m=\u001b[39m initial_w\n\u001b[1;32m     47\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_iters):\n\u001b[1;32m     48\u001b[0m     \n\u001b[1;32m     49\u001b[0m     \u001b[39m# compute loss, gradient\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     loss \u001b[39m=\u001b[39m compute_loss(y, tx, w)\n\u001b[1;32m     51\u001b[0m     g \u001b[39m=\u001b[39m compute_gradient(y, tx, w)\n\u001b[1;32m     53\u001b[0m     \u001b[39m# update w by gradient\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/EPFL/Year_I/MachineLearning/Projects/ML-Higgs-EPFL/utils.py:33\u001b[0m, in \u001b[0;36mcompute_loss\u001b[0;34m(y, tx, w, loss_type)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_loss\u001b[39m(y, tx, w, loss_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     22\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Calculate the loss using either MSE, MAE or logistic regression cost function.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39m        the value of the loss (a scalar), corresponding to the input parameters w.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     e \u001b[39m=\u001b[39m y \u001b[39m-\u001b[39m tx\u001b[39m.\u001b[39mdot(w)\n\u001b[1;32m     35\u001b[0m     \u001b[39m# MSE loss\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[39mif\u001b[39;00m loss_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m:    \n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 802. GiB for an array with shape (328135, 328135) and data type float64"
     ]
    }
   ],
   "source": [
    "# Mean squared error gradient descent\n",
    "\n",
    "w_mean_squared_error_gd, loss_mean_squared_error_gd = mean_squared_error_gd(y_train, x_train, initial_w, max_iters, gamma)\n",
    "\n",
    "rmse_tr = np.sqrt(2 * loss_mean_squared_error_gd)\n",
    "#rmse_te = np.sqrt(2 * compute_loss(y_test, x_test, w_mean_squared_error_gd))\n",
    "\n",
    "print(\"Mean squared error gradient descent: W: {w}, Loss:{loss}\".format(w=w_mean_squared_error_gd, loss=loss_mean_squared_error_gd))\n",
    "#print(\"RMSE train: {rmse_tr}, RMSE test: {rmse_te}\".format(rmse_tr=rmse_tr, rmse_te=rmse_te))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least squares: W: [ 3.13356376e-02 -3.09741815e+00 -2.70522700e+00 -2.06801610e-01\n",
      " -1.33912480e+00  1.94835177e-01 -1.03232898e+01  1.48327058e+02\n",
      " -4.77128549e-02  4.80738057e+00 -9.37298630e+01  4.14169258e+01\n",
      "  1.95969787e+01 -1.09387125e+00 -1.14289354e-01 -4.31650570e-01\n",
      "  8.45753704e-01 -1.53106392e-01  3.67680263e-01  1.60072822e+00\n",
      "  9.50537668e-02 -2.16964200e-01 -9.17137406e+01 -9.50814758e-02\n",
      "  8.77034174e-02  1.48635485e-01 -3.04442023e-02 -2.85312478e+00\n",
      " -5.12725271e+00 -5.13349749e+00], Loss:0.33944681722221687\n",
      "RMSE train: 0.8239500193849344, RMSE test: 205.03819387349836\n"
     ]
    }
   ],
   "source": [
    "# Test Least Squares Regression using Normal Equations\n",
    "\n",
    "w_least_squares, loss_least_squares = least_squares(y_tr, x_tr)\n",
    "rmse_tr = np.sqrt(2 * loss_least_squares)\n",
    "rmse_te = np.sqrt(2 * compute_loss(y_te, x_te, w_least_squares))\n",
    "\n",
    "print(\"Least squares: W: {w}, Loss:{loss}\".format(w=w_least_squares, loss=loss_least_squares))\n",
    "print(\"RMSE train: {rmse_tr}, RMSE test: {rmse_te}\".format(rmse_tr=rmse_tr, rmse_te=rmse_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression: W: [ 0.03285406 -0.01522637 -0.01105648 -0.00676256  0.0155015   0.02760007\n",
      "  0.01531225 -0.00745091 -0.00890914 -0.01231396 -0.00753058 -0.00725283\n",
      "  0.01547001 -0.00656988 -0.00737802 -0.00738148 -0.01009919 -0.00737697\n",
      " -0.00737479 -0.00953921 -0.00737048 -0.01489131 -0.00743624  0.01443978\n",
      "  0.01488679  0.01488641  0.01422847  0.01545997  0.01545747 -0.01039903], Loss:0.4339121896746597\n",
      "RMSE train: 0.9315709201930465, RMSE test: 132.4973086708635\n"
     ]
    }
   ],
   "source": [
    "# Test Ridge Regression using Normal Equations\n",
    "\n",
    "w_ridge_regression, loss_ridge_regression = ridge_regression(y_tr, x_tr, lambda_)\n",
    "rmse_tr = np.sqrt(2 * loss_ridge_regression)\n",
    "rmse_te = np.sqrt(2 * compute_loss(y_te, x_te, w_ridge_regression))\n",
    "\n",
    "print(\"Ridge regression: W: {w}, Loss:{loss}\".format(w=w_ridge_regression, loss=loss_ridge_regression))\n",
    "print(\"RMSE train: {rmse_tr}, RMSE test: {rmse_te}\".format(rmse_tr=rmse_tr, rmse_te=rmse_te))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modern_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
