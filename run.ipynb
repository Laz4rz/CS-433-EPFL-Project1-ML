{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from helpers import *\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.expand_dims(y_train, 1)\n",
    "y_train = y_train.reshape((y_train.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (328135, 320)\n",
      "y_train shape:  (328135, 1)\n",
      "x_test shape:  (109379, 320)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train shape: \", x_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape) \n",
    "print(\"x_test shape: \", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328135, 320)\n"
     ]
    }
   ],
   "source": [
    "# Build train data\n",
    "\n",
    "import src.constants as c\n",
    "from src.data.build_data import build_train_data\n",
    "\n",
    "x_train_nonans, removed_cols = build_train_data(data=x_train, percentage=c.PERCENTAGE_NAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "lambda_ = 0.1                                      # regularization parameter\n",
    "max_iters = 50                                    # max number of iterations \n",
    "threshold = 1e-8                                   # threshold for stopping criterion\n",
    "gamma = 0.4                                        # step size\n",
    "initial_w = np.zeros((x_train_nonans.shape[1], 1)) # initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error gradient descent: W: [[ 5.68090924e+92]\n",
      " [ 5.68090756e+92]\n",
      " [ 3.63064193e+92]\n",
      " [ 5.68090755e+92]\n",
      " [ 5.68090502e+92]\n",
      " [ 5.68028012e+92]\n",
      " [ 5.68056125e+92]\n",
      " [-6.23742882e+94]\n",
      " [-6.23742882e+94]\n",
      " [ 5.68090924e+92]\n",
      " [ 5.68090924e+92]\n",
      " [ 5.68090924e+92]\n",
      " [ 5.68090907e+92]\n",
      " [ 5.68090899e+92]\n",
      " [ 5.68090930e+92]\n",
      " [ 5.68090924e+92]\n",
      " [ 5.68090924e+92]\n",
      " [ 5.68090924e+92]\n",
      " [ 5.68090908e+92]\n",
      " [ 5.68090924e+92]\n",
      " [ 5.68090922e+92]\n",
      " [ 5.68090904e+92]\n",
      " [ 5.68090879e+92]\n",
      " [ 5.68090875e+92]\n",
      " [ 5.68089058e+92]\n",
      " [ 5.68088933e+92]\n",
      " [ 5.68089212e+92]\n",
      " [ 5.68090921e+92]\n",
      " [ 5.68090911e+92]\n",
      " [ 5.68090895e+92]\n",
      " [ 5.68090906e+92]\n",
      " [ 5.68090886e+92]\n",
      " [ 5.68090918e+92]\n",
      " [ 5.68090916e+92]\n",
      " [ 5.68090910e+92]\n",
      " [ 5.68090904e+92]\n",
      " [ 5.68090893e+92]\n",
      " [ 5.68090896e+92]\n",
      " [ 5.68090909e+92]\n",
      " [ 5.68090895e+92]\n",
      " [ 5.68090895e+92]\n",
      " [ 5.68090894e+92]\n",
      " [ 5.68090902e+92]\n",
      " [ 5.68090898e+92]\n",
      " [ 5.68090893e+92]\n",
      " [ 5.68090869e+92]\n",
      " [ 5.68089253e+92]\n",
      " [ 5.68090906e+92]\n",
      " [ 5.68090884e+92]\n",
      " [ 5.68090801e+92]\n",
      " [ 5.68090912e+92]\n",
      " [ 5.68090893e+92]\n",
      " [ 5.68090917e+92]\n",
      " [ 5.68090896e+92]\n",
      " [ 5.68090832e+92]\n",
      " [ 5.68088898e+92]\n",
      " [ 5.68090326e+92]\n",
      " [ 5.68090917e+92]\n",
      " [ 5.68067995e+92]\n",
      " [ 5.68067868e+92]\n",
      " [ 5.68090892e+92]\n",
      " [ 5.68090899e+92]\n",
      " [ 5.68090896e+92]\n",
      " [ 5.68090893e+92]\n",
      " [ 5.68090894e+92]\n",
      " [ 5.68090897e+92]\n",
      " [ 5.68090893e+92]\n",
      " [ 5.68090894e+92]\n",
      " [ 5.68090905e+92]\n",
      " [ 5.68090879e+92]\n",
      " [ 5.68090910e+92]\n",
      " [ 5.68090744e+92]\n",
      " [ 5.68090862e+92]\n",
      " [ 5.68074245e+92]\n",
      " [ 5.68090847e+92]\n",
      " [ 5.68088837e+92]\n",
      " [ 5.68090781e+92]\n",
      " [ 5.68079470e+92]\n",
      " [ 5.68084310e+92]\n",
      " [ 5.68081132e+92]\n",
      " [ 5.68082857e+92]\n",
      " [ 5.68081533e+92]\n",
      " [ 5.68083866e+92]\n",
      " [ 5.68090914e+92]\n",
      " [ 5.68089316e+92]\n",
      " [ 5.68086495e+92]\n",
      " [ 5.68087609e+92]\n",
      " [ 5.68088998e+92]\n",
      " [ 5.68086363e+92]\n",
      " [ 5.68087083e+92]\n",
      " [ 5.68072135e+92]\n",
      " [ 5.68090906e+92]\n",
      " [ 5.68090897e+92]\n",
      " [ 5.68090879e+92]\n",
      " [ 5.68090762e+92]\n",
      " [ 5.68090915e+92]\n",
      " [ 5.68090906e+92]\n",
      " [ 5.63823251e+92]\n",
      " [ 5.68090826e+92]\n",
      " [ 5.68090890e+92]\n",
      " [ 5.68090895e+92]\n",
      " [ 5.55940958e+92]\n",
      " [ 5.68090812e+92]\n",
      " [ 5.68090904e+92]\n",
      " [ 5.68090868e+92]\n",
      " [ 5.68090898e+92]\n",
      " [ 5.68090886e+92]\n",
      " [ 5.68090894e+92]\n",
      " [ 5.68090902e+92]\n",
      " [ 5.68090914e+92]\n",
      " [ 5.68090826e+92]\n",
      " [ 5.68090820e+92]\n",
      " [ 5.68090916e+92]\n",
      " [ 5.68090828e+92]\n",
      " [ 5.68090897e+92]\n",
      " [ 5.68090902e+92]\n",
      " [ 5.68090887e+92]\n",
      " [ 5.68090490e+92]\n",
      " [ 5.68090923e+92]\n",
      " [ 5.68090876e+92]\n",
      " [ 5.58681853e+92]\n",
      " [ 5.68087698e+92]\n",
      " [ 5.68090910e+92]\n",
      " [ 5.68087159e+92]\n",
      " [ 5.68090717e+92]\n",
      " [ 5.68090729e+92]\n",
      " [ 5.68090729e+92]\n",
      " [ 5.68065797e+92]\n",
      " [ 5.68090825e+92]\n",
      " [ 5.68090936e+92]\n",
      " [ 5.68073161e+92]\n",
      " [ 5.68090917e+92]\n",
      " [ 5.68090832e+92]\n",
      " [ 5.68090910e+92]\n",
      " [ 5.68090907e+92]\n",
      " [ 5.68090908e+92]\n",
      " [ 5.68090919e+92]\n",
      " [ 5.68090919e+92]\n",
      " [ 5.68090867e+92]\n",
      " [ 5.68090903e+92]\n",
      " [ 5.68090858e+92]\n",
      " [ 5.68090856e+92]\n",
      " [ 5.68090893e+92]\n",
      " [ 5.68090892e+92]\n",
      " [ 5.68090913e+92]\n",
      " [ 5.68090903e+92]\n",
      " [ 5.68090910e+92]\n",
      " [ 5.68090712e+92]\n",
      " [ 5.68090912e+92]\n",
      " [ 5.68089226e+92]\n",
      " [ 5.68090816e+92]\n",
      " [ 5.68088871e+92]\n",
      " [ 5.68090902e+92]\n",
      " [ 5.68088428e+92]\n",
      " [ 5.68090079e+92]\n",
      " [ 5.68090863e+92]\n",
      " [ 5.68090884e+92]\n",
      " [ 5.68090906e+92]\n",
      " [ 5.68090863e+92]\n",
      " [ 5.68090809e+92]\n",
      " [ 5.68090844e+92]\n",
      " [ 5.68090909e+92]\n",
      " [ 5.68090897e+92]\n",
      " [ 5.68089108e+92]\n",
      " [ 5.68090906e+92]\n",
      " [ 5.67902847e+92]\n",
      " [ 5.68090908e+92]\n",
      " [ 5.68090944e+92]\n",
      " [ 5.68090924e+92]\n",
      " [ 5.68090946e+92]\n",
      " [ 5.68090937e+92]\n",
      " [ 5.68090946e+92]\n",
      " [ 5.68090930e+92]\n",
      " [ 5.68090950e+92]\n",
      " [ 5.68090944e+92]\n",
      " [ 5.68090927e+92]\n",
      " [ 5.68090927e+92]\n",
      " [ 5.68090912e+92]\n",
      " [ 5.68090894e+92]\n",
      " [ 5.68090889e+92]\n",
      " [ 5.68090889e+92]\n",
      " [ 5.68090924e+92]\n",
      " [ 5.68090924e+92]\n",
      " [ 5.68090952e+92]\n",
      " [ 5.68090951e+92]\n",
      " [ 5.68090895e+92]\n",
      " [ 5.68090825e+92]\n",
      " [ 5.68090865e+92]\n",
      " [ 5.68089706e+92]\n",
      " [ 5.68090430e+92]\n",
      " [ 5.68090915e+92]\n",
      " [ 5.68090928e+92]\n",
      " [ 5.68088974e+92]\n",
      " [ 5.68088716e+92]\n",
      " [ 5.68090829e+92]\n",
      " [ 5.68090857e+92]\n",
      " [ 5.68083785e+92]\n",
      " [ 5.68086902e+92]\n",
      " [ 5.68090917e+92]\n",
      " [ 5.68090929e+92]\n",
      " [ 5.68081174e+92]\n",
      " [ 5.68085289e+92]\n",
      " [ 5.68075873e+92]\n",
      " [ 5.68088390e+92]\n",
      " [ 5.68089362e+92]\n",
      " [ 5.68086859e+92]\n",
      " [ 5.68090853e+92]\n",
      " [ 5.68090881e+92]\n",
      " [ 5.68090873e+92]\n",
      " [ 5.68090866e+92]\n",
      " [ 5.68090874e+92]\n",
      " [ 5.68090879e+92]\n",
      " [ 5.68090846e+92]\n",
      " [ 5.68090869e+92]\n",
      " [ 5.68090870e+92]\n",
      " [ 5.68090867e+92]\n",
      " [ 5.68090841e+92]\n",
      " [ 5.68090898e+92]\n",
      " [ 5.68090896e+92]\n",
      " [ 5.68090883e+92]\n",
      " [ 5.68090880e+92]\n",
      " [ 5.68090894e+92]], Loss:8.715576886164893e+191\n",
      "RMSE train: 1.3202709484166417e+96\n"
     ]
    }
   ],
   "source": [
    "# Mean squared error gradient descent\n",
    "w_mean_squared_error_gd, loss_mean_squared_error_gd = mean_squared_error_gd(y_train, x_train_nonans, initial_w, max_iters, gamma)\n",
    "\n",
    "rmse_tr = np.sqrt(2 * loss_mean_squared_error_gd)\n",
    "\n",
    "print(\"Mean squared error gradient descent: W: {w}, Loss:{loss}\".format(w=w_mean_squared_error_gd, loss=loss_mean_squared_error_gd))\n",
    "print(\"RMSE train: {rmse_tr}\".format(rmse_tr=rmse_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error stochastic gradient descent: W: [[ 5.68090924e+92]\n",
      " [ 5.68090756e+92]\n",
      " [ 3.63064193e+92]\n",
      " [ 5.68090755e+92]\n",
      " [ 5.68090502e+92]\n",
      " [ 5.68028012e+92]\n",
      " [ 5.68056125e+92]\n",
      " [-6.23742882e+94]\n",
      " [-6.23742882e+94]\n",
      " [ 5.68090924e+92]\n",
      " [ 5.68090924e+92]\n",
      " [ 5.68090924e+92]\n",
      " [ 5.68090907e+92]\n",
      " [ 5.68090899e+92]\n",
      " [ 5.68090930e+92]\n",
      " [ 5.68090924e+92]\n",
      " [ 5.68090924e+92]\n",
      " [ 5.68090924e+92]\n",
      " [ 5.68090908e+92]\n",
      " [ 5.68090924e+92]\n",
      " [ 5.68090922e+92]\n",
      " [ 5.68090904e+92]\n",
      " [ 5.68090879e+92]\n",
      " [ 5.68090875e+92]\n",
      " [ 5.68089058e+92]\n",
      " [ 5.68088933e+92]\n",
      " [ 5.68089212e+92]\n",
      " [ 5.68090921e+92]\n",
      " [ 5.68090911e+92]\n",
      " [ 5.68090895e+92]\n",
      " [ 5.68090906e+92]\n",
      " [ 5.68090886e+92]\n",
      " [ 5.68090918e+92]\n",
      " [ 5.68090916e+92]\n",
      " [ 5.68090910e+92]\n",
      " [ 5.68090904e+92]\n",
      " [ 5.68090893e+92]\n",
      " [ 5.68090896e+92]\n",
      " [ 5.68090909e+92]\n",
      " [ 5.68090895e+92]\n",
      " [ 5.68090895e+92]\n",
      " [ 5.68090894e+92]\n",
      " [ 5.68090902e+92]\n",
      " [ 5.68090898e+92]\n",
      " [ 5.68090893e+92]\n",
      " [ 5.68090869e+92]\n",
      " [ 5.68089253e+92]\n",
      " [ 5.68090906e+92]\n",
      " [ 5.68090884e+92]\n",
      " [ 5.68090801e+92]\n",
      " [ 5.68090912e+92]\n",
      " [ 5.68090893e+92]\n",
      " [ 5.68090917e+92]\n",
      " [ 5.68090896e+92]\n",
      " [ 5.68090832e+92]\n",
      " [ 5.68088898e+92]\n",
      " [ 5.68090326e+92]\n",
      " [ 5.68090917e+92]\n",
      " [ 5.68067995e+92]\n",
      " [ 5.68067868e+92]\n",
      " [ 5.68090892e+92]\n",
      " [ 5.68090899e+92]\n",
      " [ 5.68090896e+92]\n",
      " [ 5.68090893e+92]\n",
      " [ 5.68090894e+92]\n",
      " [ 5.68090897e+92]\n",
      " [ 5.68090893e+92]\n",
      " [ 5.68090894e+92]\n",
      " [ 5.68090905e+92]\n",
      " [ 5.68090879e+92]\n",
      " [ 5.68090910e+92]\n",
      " [ 5.68090744e+92]\n",
      " [ 5.68090862e+92]\n",
      " [ 5.68074245e+92]\n",
      " [ 5.68090847e+92]\n",
      " [ 5.68088837e+92]\n",
      " [ 5.68090781e+92]\n",
      " [ 5.68079470e+92]\n",
      " [ 5.68084310e+92]\n",
      " [ 5.68081132e+92]\n",
      " [ 5.68082857e+92]\n",
      " [ 5.68081533e+92]\n",
      " [ 5.68083866e+92]\n",
      " [ 5.68090914e+92]\n",
      " [ 5.68089316e+92]\n",
      " [ 5.68086495e+92]\n",
      " [ 5.68087609e+92]\n",
      " [ 5.68088998e+92]\n",
      " [ 5.68086363e+92]\n",
      " [ 5.68087083e+92]\n",
      " [ 5.68072135e+92]\n",
      " [ 5.68090906e+92]\n",
      " [ 5.68090897e+92]\n",
      " [ 5.68090879e+92]\n",
      " [ 5.68090762e+92]\n",
      " [ 5.68090915e+92]\n",
      " [ 5.68090906e+92]\n",
      " [ 5.63823251e+92]\n",
      " [ 5.68090826e+92]\n",
      " [ 5.68090890e+92]\n",
      " [ 5.68090895e+92]\n",
      " [ 5.55940958e+92]\n",
      " [ 5.68090812e+92]\n",
      " [ 5.68090904e+92]\n",
      " [ 5.68090868e+92]\n",
      " [ 5.68090898e+92]\n",
      " [ 5.68090886e+92]\n",
      " [ 5.68090894e+92]\n",
      " [ 5.68090902e+92]\n",
      " [ 5.68090914e+92]\n",
      " [ 5.68090826e+92]\n",
      " [ 5.68090820e+92]\n",
      " [ 5.68090916e+92]\n",
      " [ 5.68090828e+92]\n",
      " [ 5.68090897e+92]\n",
      " [ 5.68090902e+92]\n",
      " [ 5.68090887e+92]\n",
      " [ 5.68090490e+92]\n",
      " [ 5.68090923e+92]\n",
      " [ 5.68090876e+92]\n",
      " [ 5.58681853e+92]\n",
      " [ 5.68087698e+92]\n",
      " [ 5.68090910e+92]\n",
      " [ 5.68087159e+92]\n",
      " [ 5.68090717e+92]\n",
      " [ 5.68090729e+92]\n",
      " [ 5.68090729e+92]\n",
      " [ 5.68065797e+92]\n",
      " [ 5.68090825e+92]\n",
      " [ 5.68090936e+92]\n",
      " [ 5.68073161e+92]\n",
      " [ 5.68090917e+92]\n",
      " [ 5.68090832e+92]\n",
      " [ 5.68090910e+92]\n",
      " [ 5.68090907e+92]\n",
      " [ 5.68090908e+92]\n",
      " [ 5.68090919e+92]\n",
      " [ 5.68090919e+92]\n",
      " [ 5.68090867e+92]\n",
      " [ 5.68090903e+92]\n",
      " [ 5.68090858e+92]\n",
      " [ 5.68090856e+92]\n",
      " [ 5.68090893e+92]\n",
      " [ 5.68090892e+92]\n",
      " [ 5.68090913e+92]\n",
      " [ 5.68090903e+92]\n",
      " [ 5.68090910e+92]\n",
      " [ 5.68090712e+92]\n",
      " [ 5.68090912e+92]\n",
      " [ 5.68089226e+92]\n",
      " [ 5.68090816e+92]\n",
      " [ 5.68088871e+92]\n",
      " [ 5.68090902e+92]\n",
      " [ 5.68088428e+92]\n",
      " [ 5.68090079e+92]\n",
      " [ 5.68090863e+92]\n",
      " [ 5.68090884e+92]\n",
      " [ 5.68090906e+92]\n",
      " [ 5.68090863e+92]\n",
      " [ 5.68090809e+92]\n",
      " [ 5.68090844e+92]\n",
      " [ 5.68090909e+92]\n",
      " [ 5.68090897e+92]\n",
      " [ 5.68089108e+92]\n",
      " [ 5.68090906e+92]\n",
      " [ 5.67902847e+92]\n",
      " [ 5.68090908e+92]\n",
      " [ 5.68090944e+92]\n",
      " [ 5.68090924e+92]\n",
      " [ 5.68090946e+92]\n",
      " [ 5.68090937e+92]\n",
      " [ 5.68090946e+92]\n",
      " [ 5.68090930e+92]\n",
      " [ 5.68090950e+92]\n",
      " [ 5.68090944e+92]\n",
      " [ 5.68090927e+92]\n",
      " [ 5.68090927e+92]\n",
      " [ 5.68090912e+92]\n",
      " [ 5.68090894e+92]\n",
      " [ 5.68090889e+92]\n",
      " [ 5.68090889e+92]\n",
      " [ 5.68090924e+92]\n",
      " [ 5.68090924e+92]\n",
      " [ 5.68090952e+92]\n",
      " [ 5.68090951e+92]\n",
      " [ 5.68090895e+92]\n",
      " [ 5.68090825e+92]\n",
      " [ 5.68090865e+92]\n",
      " [ 5.68089706e+92]\n",
      " [ 5.68090430e+92]\n",
      " [ 5.68090915e+92]\n",
      " [ 5.68090928e+92]\n",
      " [ 5.68088974e+92]\n",
      " [ 5.68088716e+92]\n",
      " [ 5.68090829e+92]\n",
      " [ 5.68090857e+92]\n",
      " [ 5.68083785e+92]\n",
      " [ 5.68086902e+92]\n",
      " [ 5.68090917e+92]\n",
      " [ 5.68090929e+92]\n",
      " [ 5.68081174e+92]\n",
      " [ 5.68085289e+92]\n",
      " [ 5.68075873e+92]\n",
      " [ 5.68088390e+92]\n",
      " [ 5.68089362e+92]\n",
      " [ 5.68086859e+92]\n",
      " [ 5.68090853e+92]\n",
      " [ 5.68090881e+92]\n",
      " [ 5.68090873e+92]\n",
      " [ 5.68090866e+92]\n",
      " [ 5.68090874e+92]\n",
      " [ 5.68090879e+92]\n",
      " [ 5.68090846e+92]\n",
      " [ 5.68090869e+92]\n",
      " [ 5.68090870e+92]\n",
      " [ 5.68090867e+92]\n",
      " [ 5.68090841e+92]\n",
      " [ 5.68090898e+92]\n",
      " [ 5.68090896e+92]\n",
      " [ 5.68090883e+92]\n",
      " [ 5.68090880e+92]\n",
      " [ 5.68090894e+92]], Loss:8.71557688616638e+191\n",
      "RMSE train: 1.3202709484167544e+96\n"
     ]
    }
   ],
   "source": [
    "# Mean squared error stochastic gradient descent\n",
    "w_mean_squared_error_sgd, loss_mean_squared_error_sgd = mean_squared_error_sgd(y_train, x_train_nonans, initial_w, max_iters, gamma)\n",
    "\n",
    "rmse_tr = np.sqrt(2 * loss_mean_squared_error_sgd)\n",
    "\n",
    "print(\"Mean squared error stochastic gradient descent: W: {w}, Loss:{loss}\".format(w=w_mean_squared_error_sgd, loss=loss_mean_squared_error_sgd))\n",
    "print(\"RMSE train: {rmse_tr}\".format(rmse_tr=rmse_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/drudao/Desktop/EPFL/Year_I/MachineLearning/Projects/ML-Higgs-EPFL/run.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/drudao/Desktop/EPFL/Year_I/MachineLearning/Projects/ML-Higgs-EPFL/run.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Test Least Squares Regression using Normal Equations\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/drudao/Desktop/EPFL/Year_I/MachineLearning/Projects/ML-Higgs-EPFL/run.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m w_least_squares, loss_least_squares \u001b[39m=\u001b[39m least_squares(y_train, x_train_nonans)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/drudao/Desktop/EPFL/Year_I/MachineLearning/Projects/ML-Higgs-EPFL/run.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#rmse_tr = np.sqrt(2 * loss_least_squares)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/drudao/Desktop/EPFL/Year_I/MachineLearning/Projects/ML-Higgs-EPFL/run.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLeast squares: W: \u001b[39m\u001b[39m{w}\u001b[39;00m\u001b[39m, Loss:\u001b[39m\u001b[39m{loss}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(w\u001b[39m=\u001b[39mw_least_squares, loss\u001b[39m=\u001b[39mloss_least_squares))\n",
      "File \u001b[0;32m~/Desktop/EPFL/Year_I/MachineLearning/Projects/ML-Higgs-EPFL/implementations.py:79\u001b[0m, in \u001b[0;36mleast_squares\u001b[0;34m(y, tx)\u001b[0m\n\u001b[1;32m     76\u001b[0m a \u001b[39m=\u001b[39m tx\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mdot(tx)\n\u001b[1;32m     77\u001b[0m b \u001b[39m=\u001b[39m tx\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mdot(y)\n\u001b[0;32m---> 79\u001b[0m w \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39msolve(a, b)\n\u001b[1;32m     80\u001b[0m loss \u001b[39m=\u001b[39m compute_loss(y, tx, w)\n\u001b[1;32m     81\u001b[0m \u001b[39mreturn\u001b[39;00m w, loss\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msolve\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:386\u001b[0m, in \u001b[0;36msolve\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m    384\u001b[0m signature \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mDD->D\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m isComplexType(t) \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mdd->d\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    385\u001b[0m extobj \u001b[39m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[0;32m--> 386\u001b[0m r \u001b[39m=\u001b[39m gufunc(a, b, signature\u001b[39m=\u001b[39msignature, extobj\u001b[39m=\u001b[39mextobj)\n\u001b[1;32m    388\u001b[0m \u001b[39mreturn\u001b[39;00m wrap(r\u001b[39m.\u001b[39mastype(result_t, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/linalg/linalg.py:89\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[0;32m---> 89\u001b[0m     \u001b[39mraise\u001b[39;00m LinAlgError(\u001b[39m\"\u001b[39m\u001b[39mSingular matrix\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "# Test Least Squares Regression using Normal Equations\n",
    "\n",
    "w_least_squares, loss_least_squares = least_squares(y_train, x_train_nonans)\n",
    "\n",
    "rmse_tr = np.sqrt(2 * loss_least_squares)\n",
    "\n",
    "print(\"Least squares: W: {w}, Loss:{loss}\".format(w=w_least_squares, loss=loss_least_squares))\n",
    "print(\"RMSE train: {rmse_tr}\".format(rmse_tr=rmse_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression: W: [[-3.79799091e-05]\n",
      " [-3.79797249e-05]\n",
      " [ 1.16986226e-04]\n",
      " [-3.79797414e-05]\n",
      " [-3.79813997e-05]\n",
      " [-3.79757193e-05]\n",
      " [-3.79822931e-05]\n",
      " [ 4.16850488e-03]\n",
      " [ 4.16850488e-03]\n",
      " [-3.79799091e-05]\n",
      " [-3.79799090e-05]\n",
      " [-3.79799091e-05]\n",
      " [-3.79798791e-05]\n",
      " [-3.79801369e-05]\n",
      " [-3.79799742e-05]\n",
      " [-3.79800735e-05]\n",
      " [-3.79799091e-05]\n",
      " [-3.79799091e-05]\n",
      " [-3.79799945e-05]\n",
      " [-3.79799127e-05]\n",
      " [-3.79799244e-05]\n",
      " [-3.79799647e-05]\n",
      " [-3.79800227e-05]\n",
      " [-3.79778770e-05]\n",
      " [-3.80066694e-05]\n",
      " [-3.79819500e-05]\n",
      " [-3.79889414e-05]\n",
      " [-3.79800056e-05]\n",
      " [-3.79801672e-05]\n",
      " [-3.79799308e-05]\n",
      " [-3.79803943e-05]\n",
      " [-3.79814801e-05]\n",
      " [-3.79800511e-05]\n",
      " [-3.79802546e-05]\n",
      " [-3.79804086e-05]\n",
      " [-3.79804689e-05]\n",
      " [-3.79801427e-05]\n",
      " [-3.79800124e-05]\n",
      " [-3.79799537e-05]\n",
      " [-3.79800729e-05]\n",
      " [-3.79800767e-05]\n",
      " [-3.79801898e-05]\n",
      " [-3.79804763e-05]\n",
      " [-3.79800737e-05]\n",
      " [-3.79800337e-05]\n",
      " [-3.79807632e-05]\n",
      " [-3.79786418e-05]\n",
      " [-3.79801691e-05]\n",
      " [-3.79802835e-05]\n",
      " [-3.79805668e-05]\n",
      " [-3.79799812e-05]\n",
      " [-3.79799065e-05]\n",
      " [-3.79797925e-05]\n",
      " [-3.79802107e-05]\n",
      " [-3.79754714e-05]\n",
      " [-3.79476928e-05]\n",
      " [-3.79795649e-05]\n",
      " [-3.79794501e-05]\n",
      " [-3.83779856e-05]\n",
      " [-3.81261343e-05]\n",
      " [-3.79799046e-05]\n",
      " [-3.79804640e-05]\n",
      " [-3.79803525e-05]\n",
      " [-3.79800483e-05]\n",
      " [-3.79800814e-05]\n",
      " [-3.79804656e-05]\n",
      " [-3.79800696e-05]\n",
      " [-3.79801457e-05]\n",
      " [-3.79803068e-05]\n",
      " [-3.79797657e-05]\n",
      " [-3.79799323e-05]\n",
      " [-3.79797583e-05]\n",
      " [-3.79799174e-05]\n",
      " [-3.77550673e-05]\n",
      " [-3.79797296e-05]\n",
      " [-3.79740170e-05]\n",
      " [-3.79800504e-05]\n",
      " [-3.79814966e-05]\n",
      " [-3.79514950e-05]\n",
      " [-3.79547186e-05]\n",
      " [-3.79317251e-05]\n",
      " [-3.79610743e-05]\n",
      " [-3.79612601e-05]\n",
      " [-3.79796729e-05]\n",
      " [-3.79764967e-05]\n",
      " [-3.79692871e-05]\n",
      " [-3.79741510e-05]\n",
      " [-3.79736927e-05]\n",
      " [-3.79711992e-05]\n",
      " [-3.79653383e-05]\n",
      " [-3.78141744e-05]\n",
      " [-3.79800577e-05]\n",
      " [-3.79799302e-05]\n",
      " [-3.79801546e-05]\n",
      " [-3.79786573e-05]\n",
      " [-3.79797755e-05]\n",
      " [-3.79801897e-05]\n",
      " [-3.36303645e-05]\n",
      " [-3.79806927e-05]\n",
      " [-3.79810735e-05]\n",
      " [-3.79796966e-05]\n",
      " [-8.87147765e-06]\n",
      " [-3.79796626e-05]\n",
      " [-3.79799516e-05]\n",
      " [-3.79799597e-05]\n",
      " [-3.79799015e-05]\n",
      " [-3.79798614e-05]\n",
      " [-3.79799692e-05]\n",
      " [-3.79799157e-05]\n",
      " [-3.79799167e-05]\n",
      " [-3.79799603e-05]\n",
      " [-3.79799868e-05]\n",
      " [-3.79798623e-05]\n",
      " [-3.79798863e-05]\n",
      " [-3.79799067e-05]\n",
      " [-3.79798695e-05]\n",
      " [-3.79798997e-05]\n",
      " [-3.79828461e-05]\n",
      " [-3.79799222e-05]\n",
      " [-3.79796992e-05]\n",
      " [-3.09127570e-05]\n",
      " [-3.80305673e-05]\n",
      " [-3.79799191e-05]\n",
      " [-3.80305696e-05]\n",
      " [-3.79784499e-05]\n",
      " [-3.79797942e-05]\n",
      " [-3.79797942e-05]\n",
      " [-3.79864858e-05]\n",
      " [-3.79804268e-05]\n",
      " [-3.79798857e-05]\n",
      " [-3.83319281e-05]\n",
      " [-3.79791912e-05]\n",
      " [-3.79743488e-05]\n",
      " [-3.79791000e-05]\n",
      " [-3.79805029e-05]\n",
      " [-3.79792602e-05]\n",
      " [-3.79797508e-05]\n",
      " [-3.79797388e-05]\n",
      " [-3.79801124e-05]\n",
      " [-3.79804964e-05]\n",
      " [-3.79807023e-05]\n",
      " [-3.79806621e-05]\n",
      " [-3.79798367e-05]\n",
      " [-3.79803250e-05]\n",
      " [-3.79799573e-05]\n",
      " [-3.79801405e-05]\n",
      " [-3.79800783e-05]\n",
      " [-3.79741914e-05]\n",
      " [-3.79792092e-05]\n",
      " [-3.79512811e-05]\n",
      " [-3.79775074e-05]\n",
      " [-3.79794216e-05]\n",
      " [-3.79798987e-05]\n",
      " [-3.79716726e-05]\n",
      " [-3.79773462e-05]\n",
      " [-3.79795527e-05]\n",
      " [-3.79801547e-05]\n",
      " [-3.79807544e-05]\n",
      " [-3.79805189e-05]\n",
      " [-3.79808327e-05]\n",
      " [-3.79804379e-05]\n",
      " [-3.79799426e-05]\n",
      " [-3.79796796e-05]\n",
      " [-3.79974649e-05]\n",
      " [-3.79801571e-05]\n",
      " [-3.94557572e-05]\n",
      " [-3.79800580e-05]\n",
      " [-3.79798067e-05]\n",
      " [-3.79801573e-05]\n",
      " [-3.79799340e-05]\n",
      " [-3.79801124e-05]\n",
      " [-3.79799509e-05]\n",
      " [-3.79800047e-05]\n",
      " [-3.79799224e-05]\n",
      " [-3.79799212e-05]\n",
      " [-3.79799120e-05]\n",
      " [-3.79799298e-05]\n",
      " [-3.79800554e-05]\n",
      " [-3.79802586e-05]\n",
      " [-3.79798432e-05]\n",
      " [-3.79796638e-05]\n",
      " [-3.79799094e-05]\n",
      " [-3.79799091e-05]\n",
      " [-3.79799079e-05]\n",
      " [-3.79798906e-05]\n",
      " [-3.79797892e-05]\n",
      " [-3.79800909e-05]\n",
      " [-3.79803318e-05]\n",
      " [-3.80030594e-05]\n",
      " [-3.79920852e-05]\n",
      " [-3.79796331e-05]\n",
      " [-3.79798434e-05]\n",
      " [-3.79834271e-05]\n",
      " [-3.79764175e-05]\n",
      " [-3.79795263e-05]\n",
      " [-3.79795898e-05]\n",
      " [-3.79680906e-05]\n",
      " [-3.79723771e-05]\n",
      " [-3.79803344e-05]\n",
      " [-3.79800441e-05]\n",
      " [-3.79113479e-05]\n",
      " [-3.79364978e-05]\n",
      " [-3.78756087e-05]\n",
      " [-3.79233669e-05]\n",
      " [-3.79440176e-05]\n",
      " [-3.78892689e-05]\n",
      " [-3.79795138e-05]\n",
      " [-3.79798299e-05]\n",
      " [-3.79795796e-05]\n",
      " [-3.79796718e-05]\n",
      " [-3.79799222e-05]\n",
      " [-3.79798790e-05]\n",
      " [-3.79795003e-05]\n",
      " [-3.79798228e-05]\n",
      " [-3.79805425e-05]\n",
      " [-3.79802306e-05]\n",
      " [-3.79807396e-05]\n",
      " [-3.79799732e-05]\n",
      " [-3.79799762e-05]\n",
      " [-3.79801724e-05]\n",
      " [-3.79802909e-05]\n",
      " [-3.79796581e-05]], Loss:0.04025241008612881\n",
      "RMSE train: 0.28373371349252385\n"
     ]
    }
   ],
   "source": [
    "# Test Ridge Regression using Normal Equations\n",
    "\n",
    "w_ridge_regression, loss_ridge_regression = ridge_regression(y_train, x_train_nonans, lambda_)\n",
    "\n",
    "rmse_tr = np.sqrt(2 * loss_ridge_regression)\n",
    "\n",
    "print(\"Ridge regression: W: {w}, Loss:{loss}\".format(w=w_ridge_regression, loss=loss_ridge_regression))\n",
    "print(\"RMSE train: {rmse_tr}\".format(rmse_tr=rmse_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression: W: [[ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.00945532]\n",
      " [ 0.01390955]\n",
      " [ 0.01390954]\n",
      " [ 0.01390801]\n",
      " [ 0.01390868]\n",
      " [-1.527224  ]\n",
      " [-1.527224  ]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390956]\n",
      " [ 0.0139094 ]\n",
      " [ 0.0139095 ]\n",
      " [ 0.01390948]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390952]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390957]\n",
      " [ 0.01390963]\n",
      " [ 0.01390954]\n",
      " [ 0.01390956]\n",
      " [ 0.01390739]\n",
      " [ 0.0139084 ]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01391004]\n",
      " [ 0.01390955]\n",
      " [ 0.01390953]\n",
      " [ 0.01390955]\n",
      " [ 0.01390926]\n",
      " [ 0.0139095 ]\n",
      " [ 0.01390941]\n",
      " [ 0.01390955]\n",
      " [ 0.0139094 ]\n",
      " [ 0.01390945]\n",
      " [ 0.01390956]\n",
      " [ 0.01390953]\n",
      " [ 0.01390949]\n",
      " [ 0.0139095 ]\n",
      " [ 0.01390953]\n",
      " [ 0.01390948]\n",
      " [ 0.01390952]\n",
      " [ 0.01390975]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390956]\n",
      " [ 0.01390956]\n",
      " [ 0.01390955]\n",
      " [ 0.01382132]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01372525]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390953]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01370493]\n",
      " [ 0.01390927]\n",
      " [ 0.01390955]\n",
      " [ 0.01390926]\n",
      " [ 0.01390956]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390891]\n",
      " [ 0.01390955]\n",
      " [ 0.01390956]\n",
      " [ 0.01390771]\n",
      " [ 0.01390956]\n",
      " [ 0.01390957]\n",
      " [ 0.01390956]\n",
      " [ 0.01390955]\n",
      " [ 0.01390956]\n",
      " [ 0.01390956]\n",
      " [ 0.01390956]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390957]\n",
      " [ 0.01390956]\n",
      " [ 0.01390963]\n",
      " [ 0.01390956]\n",
      " [ 0.01390951]\n",
      " [ 0.01390955]\n",
      " [ 0.01390953]\n",
      " [ 0.01390954]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390944]\n",
      " [ 0.01390955]\n",
      " [ 0.013899  ]\n",
      " [ 0.01390955]\n",
      " [ 0.01390956]\n",
      " [ 0.01390955]\n",
      " [ 0.01390956]\n",
      " [ 0.01390955]\n",
      " [ 0.01390956]\n",
      " [ 0.01390955]\n",
      " [ 0.01390956]\n",
      " [ 0.01390956]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390956]\n",
      " [ 0.01390956]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390943]\n",
      " [ 0.01390949]\n",
      " [ 0.01390956]\n",
      " [ 0.01390956]\n",
      " [ 0.01390949]\n",
      " [ 0.01390951]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390943]\n",
      " [ 0.01390949]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390959]\n",
      " [ 0.01390959]\n",
      " [ 0.0139096 ]\n",
      " [ 0.01390972]\n",
      " [ 0.01390966]\n",
      " [ 0.01390982]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]\n",
      " [ 0.01390955]], Loss:2.854509738092351\n",
      "RMSE train: 2.3893554520382065\n"
     ]
    }
   ],
   "source": [
    "# Test Logistic Regression using gd\n",
    "\n",
    "w_log_regression, loss_log_regression = logistic_regression(y_train, x_train_nonans, initial_w, max_iters, gamma)\n",
    "\n",
    "rmse_tr = np.sqrt(2 * loss_log_regression)\n",
    "\n",
    "print(\"Logistic regression: W: {w}, Loss:{loss}\".format(w=w_log_regression, loss=loss_log_regression))\n",
    "print(\"RMSE train: {rmse_tr}\".format(rmse_tr=rmse_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression: W: [[ 0.02525463]\n",
      " [ 0.02525462]\n",
      " [ 0.01627935]\n",
      " [ 0.02525462]\n",
      " [ 0.02525461]\n",
      " [ 0.02525183]\n",
      " [ 0.02525308]\n",
      " [-2.77286642]\n",
      " [-2.77286642]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525462]\n",
      " [ 0.02525463]\n",
      " [ 0.02525452]\n",
      " [ 0.02525454]\n",
      " [ 0.02525454]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525462]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525462]\n",
      " [ 0.02525455]\n",
      " [ 0.02525463]\n",
      " [ 0.02525462]\n",
      " [ 0.02525462]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525457]\n",
      " [ 0.0252546 ]\n",
      " [ 0.02525463]\n",
      " [ 0.02525321]\n",
      " [ 0.02525346]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525462]\n",
      " [ 0.02525462]\n",
      " [ 0.02525411]\n",
      " [ 0.02525462]\n",
      " [ 0.02525454]\n",
      " [ 0.02525462]\n",
      " [ 0.02525412]\n",
      " [ 0.02525436]\n",
      " [ 0.02525422]\n",
      " [ 0.02525432]\n",
      " [ 0.02525423]\n",
      " [ 0.02525433]\n",
      " [ 0.02525463]\n",
      " [ 0.02525456]\n",
      " [ 0.02525444]\n",
      " [ 0.02525449]\n",
      " [ 0.02525455]\n",
      " [ 0.02525443]\n",
      " [ 0.02525447]\n",
      " [ 0.02525395]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525462]\n",
      " [ 0.02525462]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02506891]\n",
      " [ 0.02525462]\n",
      " [ 0.02525462]\n",
      " [ 0.02525463]\n",
      " [ 0.02474236]\n",
      " [ 0.02525462]\n",
      " [ 0.02525463]\n",
      " [ 0.02525462]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525462]\n",
      " [ 0.02525462]\n",
      " [ 0.02525463]\n",
      " [ 0.02525462]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.0252546 ]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02484268]\n",
      " [ 0.02525443]\n",
      " [ 0.02525463]\n",
      " [ 0.02525441]\n",
      " [ 0.02525462]\n",
      " [ 0.02525462]\n",
      " [ 0.02525462]\n",
      " [ 0.0252535 ]\n",
      " [ 0.02525462]\n",
      " [ 0.02525463]\n",
      " [ 0.02525349]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525462]\n",
      " [ 0.02525463]\n",
      " [ 0.02525462]\n",
      " [ 0.02525462]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525462]\n",
      " [ 0.02525463]\n",
      " [ 0.02525458]\n",
      " [ 0.02525462]\n",
      " [ 0.02525454]\n",
      " [ 0.02525463]\n",
      " [ 0.02525452]\n",
      " [ 0.02525459]\n",
      " [ 0.02525462]\n",
      " [ 0.02525462]\n",
      " [ 0.02525463]\n",
      " [ 0.02525462]\n",
      " [ 0.02525462]\n",
      " [ 0.02525462]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525453]\n",
      " [ 0.02525463]\n",
      " [ 0.0252448 ]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525462]\n",
      " [ 0.02525462]\n",
      " [ 0.02525455]\n",
      " [ 0.02525459]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525454]\n",
      " [ 0.02525453]\n",
      " [ 0.02525462]\n",
      " [ 0.02525462]\n",
      " [ 0.02525432]\n",
      " [ 0.02525446]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525426]\n",
      " [ 0.02525442]\n",
      " [ 0.02525406]\n",
      " [ 0.02525457]\n",
      " [ 0.02525459]\n",
      " [ 0.02525454]\n",
      " [ 0.02525462]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525462]\n",
      " [ 0.02525462]\n",
      " [ 0.02525463]\n",
      " [ 0.02525462]\n",
      " [ 0.02525462]\n",
      " [ 0.02525462]\n",
      " [ 0.02525462]\n",
      " [ 0.02525462]\n",
      " [ 0.02525463]\n",
      " [ 0.02525463]\n",
      " [ 0.02525462]\n",
      " [ 0.02525462]\n",
      " [ 0.02525463]], Loss:5.182713360052017\n",
      "RMSE train: 3.2195382774714814\n"
     ]
    }
   ],
   "source": [
    "# Test Regularized Logistic Regression using gd\n",
    "\n",
    "w_reg_log_regression, loss_reg_log_regression = reg_logistic_regression(y_train, x_train_nonans, lambda_, initial_w, max_iters, gamma)\n",
    "\n",
    "rmse_tr = np.sqrt(2 * loss_reg_log_regression)\n",
    "\n",
    "print(\"Logistic regression: W: {w}, Loss:{loss}\".format(w=w_reg_log_regression, loss=loss_reg_log_regression))\n",
    "print(\"RMSE train: {rmse_tr}\".format(rmse_tr=rmse_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(221,)\n"
     ]
    }
   ],
   "source": [
    "# Create submission\n",
    "\n",
    "from src.data.build_data import build_test_data\n",
    "from src.model.predictions import Models, create_submission\n",
    "\n",
    "create_submission(\n",
    "    x_test=x_test,\n",
    "    w=w_log_regression,\n",
    "    removed_cols=removed_cols,\n",
    "    model=Models.LOGISTIC,\n",
    "    filename=\"submission.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive predictions:  109379\n",
      "Negative predictions:  0\n"
     ]
    }
   ],
   "source": [
    "# Quick check on positive/negative predictions\n",
    "\n",
    "import src.constants as c\n",
    "\n",
    "pred = np.genfromtxt(\n",
    "    os.path.join(c.MODELS_PATH, \"sub.csv\"), delimiter=\",\", skip_header=1\n",
    ")\n",
    "pred = pred[:, 1]\n",
    "\n",
    "print(\"Positive predictions: \", np.sum(pred == 1))\n",
    "print(\"Negative predictions: \", np.sum(pred == -1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modern_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
