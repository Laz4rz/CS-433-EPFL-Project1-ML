{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import helpers as hp\n",
    "import implementations as impl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "x_train, x_test, y_train, train_ids, test_ids = hp.load_csv_data(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.expand_dims(y_train, 1)\n",
    "y_train = y_train.reshape((y_train.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (328135, 320)\n",
      "y_train shape:  (328135, 1)\n",
      "x_test shape:  (109379, 320)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train shape: \", x_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape) \n",
    "print(\"x_test shape: \", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build train data\n",
    "\n",
    "import src.utils.constants as c\n",
    "import src.features.build_features as bf\n",
    "x_train_nonans, removed_cols = bf.build_train_features(data=x_train, percentage=c.PERCENTAGE_NAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "lambda_ = 0.1                                      # regularization parameter\n",
    "max_iters = 10                                     # max number of iterations \n",
    "threshold = 1e-8                                   # threshold for stopping criterion\n",
    "gamma = 0.4                                        # step size\n",
    "initial_w = np.zeros((x_train_nonans.shape[1], 1)) # initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean squared error gradient descent\n",
    "w_mean_squared_error_gd, loss_mean_squared_error_gd = impl.mean_squared_error_gd(y_train, x_train_nonans, initial_w, max_iters, gamma)\n",
    "\n",
    "rmse_tr = np.sqrt(2 * loss_mean_squared_error_gd)\n",
    "\n",
    "print(\"Mean squared error gradient descent: W: {w}, Loss:{loss}\".format(w=w_mean_squared_error_gd, loss=loss_mean_squared_error_gd))\n",
    "print(\"RMSE train: {rmse_tr}\".format(rmse_tr=rmse_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean squared error stochastic gradient descent\n",
    "w_mean_squared_error_sgd, loss_mean_squared_error_sgd = impl.mean_squared_error_sgd(y_train, x_train_nonans, initial_w, max_iters, gamma)\n",
    "\n",
    "rmse_tr = np.sqrt(2 * loss_mean_squared_error_sgd)\n",
    "\n",
    "print(\"Mean squared error stochastic gradient descent: W: {w}, Loss:{loss}\".format(w=w_mean_squared_error_sgd, loss=loss_mean_squared_error_sgd))\n",
    "print(\"RMSE train: {rmse_tr}\".format(rmse_tr=rmse_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Least Squares Regression using Normal Equations\n",
    "\n",
    "w_least_squares, loss_least_squares = impl.least_squares(y_train, x_train_nonans)\n",
    "\n",
    "rmse_tr = np.sqrt(2 * loss_least_squares)\n",
    "\n",
    "print(\"Least squares: W: {w}, Loss:{loss}\".format(w=w_least_squares, loss=loss_least_squares))\n",
    "print(\"RMSE train: {rmse_tr}\".format(rmse_tr=rmse_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Ridge Regression using Normal Equations\n",
    "\n",
    "w_ridge_regression, loss_ridge_regression = impl.ridge_regression(y_train, x_train_nonans, lambda_)\n",
    "\n",
    "rmse_tr = np.sqrt(2 * loss_ridge_regression)\n",
    "\n",
    "print(\"Ridge regression: W: {w}, Loss:{loss}\".format(w=w_ridge_regression, loss=loss_ridge_regression))\n",
    "print(\"RMSE train: {rmse_tr}\".format(rmse_tr=rmse_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression: W: [[ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01278254]\n",
      " [ 0.01982394]\n",
      " [ 0.01982393]\n",
      " [ 0.01982175]\n",
      " [ 0.01982272]\n",
      " [-2.17659702]\n",
      " [-2.17659702]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982386]\n",
      " [ 0.01982387]\n",
      " [ 0.01982388]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982389]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.0198239 ]\n",
      " [ 0.01982392]\n",
      " [ 0.01982394]\n",
      " [ 0.01982282]\n",
      " [ 0.01982302]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982354]\n",
      " [ 0.01982394]\n",
      " [ 0.01982387]\n",
      " [ 0.01982394]\n",
      " [ 0.01982354]\n",
      " [ 0.01982373]\n",
      " [ 0.01982362]\n",
      " [ 0.0198237 ]\n",
      " [ 0.01982363]\n",
      " [ 0.01982371]\n",
      " [ 0.01982394]\n",
      " [ 0.01982389]\n",
      " [ 0.0198238 ]\n",
      " [ 0.01982383]\n",
      " [ 0.01982388]\n",
      " [ 0.01982379]\n",
      " [ 0.01982382]\n",
      " [ 0.01982342]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01967827]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.0194226 ]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982393]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01950076]\n",
      " [ 0.01982379]\n",
      " [ 0.01982394]\n",
      " [ 0.01982377]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982306]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982304]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982391]\n",
      " [ 0.01982394]\n",
      " [ 0.01982387]\n",
      " [ 0.01982394]\n",
      " [ 0.01982386]\n",
      " [ 0.01982392]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982387]\n",
      " [ 0.01982394]\n",
      " [ 0.01981619]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982388]\n",
      " [ 0.01982392]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982387]\n",
      " [ 0.01982387]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.0198237 ]\n",
      " [ 0.01982381]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982366]\n",
      " [ 0.01982378]\n",
      " [ 0.0198235 ]\n",
      " [ 0.0198239 ]\n",
      " [ 0.01982392]\n",
      " [ 0.01982387]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]\n",
      " [ 0.01982394]], Loss:4.068237274099001\n",
      "RMSE train: 2.852450621517926\n"
     ]
    }
   ],
   "source": [
    "# Test Logistic Regression using gd\n",
    "\n",
    "w_log_regression, loss_log_regression = impl.logistic_regression(y_train, x_train_nonans, initial_w, max_iters, gamma)\n",
    "\n",
    "rmse_tr = np.sqrt(2 * loss_log_regression)\n",
    "\n",
    "print(\"Logistic regression: W: {w}, Loss:{loss}\".format(w=w_log_regression, loss=loss_log_regression))\n",
    "print(\"RMSE train: {rmse_tr}\".format(rmse_tr=rmse_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Regularized Logistic Regression using gd\n",
    "\n",
    "w_reg_log_regression, loss_reg_log_regression = impl.reg_logistic_regression(y_train, x_train_nonans, lambda_, initial_w, max_iters, gamma)\n",
    "\n",
    "rmse_tr = np.sqrt(2 * loss_reg_log_regression)\n",
    "\n",
    "print(\"Logistic regression: W: {w}, Loss:{loss}\".format(w=w_reg_log_regression, loss=loss_reg_log_regression))\n",
    "print(\"RMSE train: {rmse_tr}\".format(rmse_tr=rmse_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0, F1: 0.0, W: [[ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00493151]\n",
      " [ 0.00759127]\n",
      " [ 0.00759126]\n",
      " [ 0.00759043]\n",
      " [ 0.0075908 ]\n",
      " [-0.8334944 ]\n",
      " [-0.8334944 ]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759123]\n",
      " [ 0.00759124]\n",
      " [ 0.00759124]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759125]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759126]\n",
      " [ 0.00759126]\n",
      " [ 0.00759127]\n",
      " [ 0.00759074]\n",
      " [ 0.00759088]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759117]\n",
      " [ 0.00759127]\n",
      " [ 0.00759125]\n",
      " [ 0.00759127]\n",
      " [ 0.00759112]\n",
      " [ 0.0075912 ]\n",
      " [ 0.00759115]\n",
      " [ 0.00759119]\n",
      " [ 0.00759116]\n",
      " [ 0.00759119]\n",
      " [ 0.00759127]\n",
      " [ 0.00759125]\n",
      " [ 0.00759122]\n",
      " [ 0.00759123]\n",
      " [ 0.00759125]\n",
      " [ 0.00759121]\n",
      " [ 0.00759123]\n",
      " [ 0.00759111]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00753654]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00744492]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759126]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00746918]\n",
      " [ 0.0075912 ]\n",
      " [ 0.00759127]\n",
      " [ 0.00759119]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759093]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759083]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759126]\n",
      " [ 0.00759127]\n",
      " [ 0.00759124]\n",
      " [ 0.00759127]\n",
      " [ 0.00759124]\n",
      " [ 0.00759126]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759124]\n",
      " [ 0.00759127]\n",
      " [ 0.00758792]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759124]\n",
      " [ 0.00759126]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759124]\n",
      " [ 0.00759124]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759118]\n",
      " [ 0.00759122]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759118]\n",
      " [ 0.00759122]\n",
      " [ 0.00759113]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]\n",
      " [ 0.00759127]]\n"
     ]
    }
   ],
   "source": [
    "# Cross validation\n",
    "\n",
    "import src.model.Models as model\n",
    "import src.model.train_model as tm\n",
    "\n",
    "kwargs = {\n",
    "    \"lambda_\": lambda_,\n",
    "    \"initial_w\": initial_w,\n",
    "    \"max_iters\": max_iters,\n",
    "    \"gamma\": gamma,\n",
    "}\n",
    "\n",
    "accuracy, f1, w = tm.run_cross_validation(\n",
    "    x=x_train_nonans,\n",
    "    y=y_train,\n",
    "    k=6,\n",
    "    algorithm=impl.reg_logistic_regression,\n",
    "    model=model.Models.LOGISTIC,\n",
    "    **kwargs,\n",
    ")\n",
    "\n",
    "print(\"Accuracy: {accuracy}, F1: {f1}, W: {w}\".format(accuracy=accuracy, f1=f1, w=w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission\n",
    "\n",
    "import src.utils.functions as utils\n",
    "\n",
    "utils.create_submission(\n",
    "    x_test=x_test,\n",
    "    w=w_log_regression,\n",
    "    removed_cols=removed_cols,\n",
    "    model=model.Models.LOGISTIC,\n",
    "    filename=\"submission.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check on positive/negative predictions\n",
    "\n",
    "import os\n",
    "\n",
    "pred = np.genfromtxt(\n",
    "    os.path.join(c.MODELS_PATH, \"sub.csv\"), delimiter=\",\", skip_header=1\n",
    ")\n",
    "pred = pred[:, 1]\n",
    "\n",
    "print(\"Positive predictions: \", np.sum(pred == 1))\n",
    "print(\"Negative predictions: \", np.sum(pred == -1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modern_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
